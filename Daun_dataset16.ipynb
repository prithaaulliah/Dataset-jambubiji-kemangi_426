{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/Dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Zd9V63kfnj-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/tmp/Dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'Train')\n",
        "val_dir = os.path.join(base_dir, 'Validation')"
      ],
      "metadata": {
        "id": "mcamlaPynvJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validasi Dataset"
      ],
      "metadata": {
        "id": "QkInoMxdn9aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kemangi_train_path = train_dir + '/kemangi'\n",
        "jambu_train_path = train_dir + '/Jambu_biji'\n",
        "kemangi_val_path = val_dir + '/kemangi'\n",
        "jambu_val_path = val_dir + '/Jambu_Biji'\n",
        "\n",
        "kemangi_len_train = len(os.listdir(kemangi_train_path))\n",
        "jambu_len_train = len(os.listdir(jambu_train_path))\n",
        "kemangi_len_val = len(os.listdir(kemangi_val_path))\n",
        "jambu_len_val = len(os.listdir(jambu_val_path))\n",
        "\n",
        "print(\"jumlah dataset Training : \", kemangi_len_train + jambu_len_train)\n",
        "print(\"jumlah dataset validasi : \", kemangi_len_val + jambu_len_val)\n",
        "print(\"\\n\\n\")\n",
        "print(\"jumlah train kelas kemangi : \", kemangi_len_train)\n",
        "print(\"jumlah train kelas Jambu : \", jambu_len_train)\n",
        "print(\"jumlah validasi kelas kemangi : \", kemangi_len_val)\n",
        "print(\"jumlah validasi kelas Jambu : \", jambu_len_val)"
      ],
      "metadata": {
        "id": "oFjgF0IVoABW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "tQHlaVzyoF9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentasi"
      ],
      "metadata": {
        "id": "A_aws41loHH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "_CYyKNCjoJPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4423a8-86d2-43a4-bac2-5e7f20101b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arsitektur Model"
      ],
      "metadata": {
        "id": "HMShdzDnoqLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Import Library yang dibutuhkan\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Flatten,Dropout, BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "2nIcTHw_ouCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "definisikan dan setting callback untuk :\n",
        "1. memantau performa model\n",
        "2. melakukan save model terbaik menggunakan model checkpoint\n",
        "3. memberhentikan pelatihan ketika tidak memnuhi syarat dalam parameter earlystopping\n",
        "'''\n",
        "\n",
        "callbacks = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='auto')        \n",
        "directory_to_save_best_model_file = '/content/drive/MyDrive/Colab Notebooks/Modul Kecerdasan Buatan Pak Galih /7. Transfer Learning /model_drop_batch_weight_from_callback_2.h5'\n",
        "best_model = ModelCheckpoint(directory_to_save_best_model_file, monitor='val_accuracy', verbose = 1, save_best_only = True)"
      ],
      "metadata": {
        "id": "4BsXL7H7owhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "definisikan pretrained model yang ingin digunakan\n",
        "'''\n",
        "\n",
        "VGG16_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', #include_top = false , berarti fully connected layer akan dipidah dari arsitektur\n",
        "                                                 input_tensor=None, input_shape=(224, 224,3))"
      ],
      "metadata": {
        "id": "6TjACsRZo_Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "melakukan setting bahwa model pretrained tidak akan dilatih ulang\n",
        "'''\n",
        "\n",
        "VGG16_base.trainable = False"
      ],
      "metadata": {
        "id": "mkyz_nWVpBZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "mendefinisikan layer pada bagian fully connected layer\n",
        "'''\n",
        "\n",
        "print('Adding new layers')\n",
        "output = VGG16_base.get_layer(index = -1).output  \n",
        "output = Flatten()(output)\n",
        "output = Dense(256,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.5)(output)\n",
        "output = Dense(256,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.5)(output)\n",
        "output = Dense(1, activation='sigmoid')(output) \n",
        "print('New layers Finishing Added!!!!')"
      ],
      "metadata": {
        "id": "pmhP7qX-pDpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c26d69-26fd-4ea8-8c7a-c2c206d6c144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding new layers\n",
            "New layers Finishing Added!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_model = Model(VGG16_base.input, output)\n",
        "\n",
        "VGG16_model.summary()"
      ],
      "metadata": {
        "id": "54H2TI29pI9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6318fa32-8aa6-47be-800d-0739ecf02912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               6422784   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,205,569\n",
            "Trainable params: 6,489,857\n",
            "Non-trainable params: 14,715,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', \n",
        "                        metrics =['accuracy'])"
      ],
      "metadata": {
        "id": "qjg6eDa3pKgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = VGG16_model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=4,  # images = batch_size * steps\n",
        "      epochs=80,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=1,  #  images = batch_size * steps\n",
        "      callbacks = [callbacks, best_model])"
      ],
      "metadata": {
        "id": "RMv-D_D3pPzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7fa22f-964f-453a-cf52-1ae97020e2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.8861 - accuracy: 0.5250\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.95000, saving model to /content/drive/MyDrive/Colab Notebooks/Modul Kecerdasan Buatan Pak Galih /7. Transfer Learning /model_drop_batch_weight_from_callback_2.h5\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.8861 - accuracy: 0.5250 - val_loss: 0.5095 - val_accuracy: 0.9500\n",
            "Epoch 2/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8625\n",
            "Epoch 00002: val_accuracy improved from 0.95000 to 1.00000, saving model to /content/drive/MyDrive/Colab Notebooks/Modul Kecerdasan Buatan Pak Galih /7. Transfer Learning /model_drop_batch_weight_from_callback_2.h5\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.3785 - accuracy: 0.8625 - val_loss: 0.3324 - val_accuracy: 1.0000\n",
            "Epoch 3/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8875\n",
            "Epoch 00003: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.3090 - accuracy: 0.8875 - val_loss: 0.2257 - val_accuracy: 1.0000\n",
            "Epoch 4/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8875\n",
            "Epoch 00004: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.2721 - accuracy: 0.8875 - val_loss: 0.1686 - val_accuracy: 1.0000\n",
            "Epoch 5/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8250\n",
            "Epoch 00005: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.3636 - accuracy: 0.8250 - val_loss: 0.1409 - val_accuracy: 1.0000\n",
            "Epoch 6/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9000\n",
            "Epoch 00006: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.2215 - accuracy: 0.9000 - val_loss: 0.1243 - val_accuracy: 1.0000\n",
            "Epoch 7/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9375\n",
            "Epoch 00007: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1512 - accuracy: 0.9375 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
            "Epoch 8/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9250\n",
            "Epoch 00008: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.2560 - accuracy: 0.9250 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
            "Epoch 9/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9375\n",
            "Epoch 00009: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1459 - accuracy: 0.9375 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
            "Epoch 10/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9750\n",
            "Epoch 00010: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0913 - accuracy: 0.9750 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
            "Epoch 11/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.8875\n",
            "Epoch 00011: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.2294 - accuracy: 0.8875 - val_loss: 0.0793 - val_accuracy: 1.0000\n",
            "Epoch 12/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9625\n",
            "Epoch 00012: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0903 - accuracy: 0.9625 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
            "Epoch 13/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9625\n",
            "Epoch 00013: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0996 - accuracy: 0.9625 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
            "Epoch 14/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9250\n",
            "Epoch 00014: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1051 - accuracy: 0.9250 - val_loss: 0.0805 - val_accuracy: 1.0000\n",
            "Epoch 15/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9500\n",
            "Epoch 00015: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1267 - accuracy: 0.9500 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
            "Epoch 16/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9750\n",
            "Epoch 00016: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0774 - accuracy: 0.9750 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
            "Epoch 17/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9500\n",
            "Epoch 00017: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1016 - accuracy: 0.9500 - val_loss: 0.0971 - val_accuracy: 1.0000\n",
            "Epoch 18/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9500\n",
            "Epoch 00018: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1373 - accuracy: 0.9500 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
            "Epoch 19/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9500\n",
            "Epoch 00019: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1307 - accuracy: 0.9500 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
            "Epoch 20/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9750\n",
            "Epoch 00020: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0777 - accuracy: 0.9750 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
            "Epoch 21/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9625\n",
            "Epoch 00021: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1488 - accuracy: 0.9625 - val_loss: 0.0549 - val_accuracy: 1.0000\n",
            "Epoch 22/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 00022: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
            "Epoch 23/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9625\n",
            "Epoch 00023: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.0563 - accuracy: 0.9625 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "Epoch 24/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9875\n",
            "Epoch 00024: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 9s 2s/step - loss: 0.0375 - accuracy: 0.9875 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
            "Epoch 25/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9875\n",
            "Epoch 00025: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.0313 - accuracy: 0.9875 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
            "Epoch 26/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9625\n",
            "Epoch 00026: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0851 - accuracy: 0.9625 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "Epoch 27/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9875\n",
            "Epoch 00027: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "Epoch 28/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9750\n",
            "Epoch 00028: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0872 - accuracy: 0.9750 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 29/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9750\n",
            "Epoch 00029: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0588 - accuracy: 0.9750 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "Epoch 30/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9875\n",
            "Epoch 00030: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0413 - accuracy: 0.9875 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "Epoch 31/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9375\n",
            "Epoch 00031: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1123 - accuracy: 0.9375 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "Epoch 32/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 00032: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "Epoch 33/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9500\n",
            "Epoch 00033: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1146 - accuracy: 0.9500 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "Epoch 34/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9875\n",
            "Epoch 00034: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0328 - accuracy: 0.9875 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "Epoch 35/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9750\n",
            "Epoch 00035: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0647 - accuracy: 0.9750 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "Epoch 36/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 00036: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 37/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9375\n",
            "Epoch 00037: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0832 - accuracy: 0.9375 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 38/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9875\n",
            "Epoch 00038: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0263 - accuracy: 0.9875 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "Epoch 39/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
            "Epoch 00039: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "Epoch 40/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9875\n",
            "Epoch 00040: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0289 - accuracy: 0.9875 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 41/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9875\n",
            "Epoch 00041: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0527 - accuracy: 0.9875 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 42/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9750\n",
            "Epoch 00042: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0635 - accuracy: 0.9750 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 43/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 00043: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 44/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9875\n",
            "Epoch 00044: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0295 - accuracy: 0.9875 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 45/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 00045: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 46/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 00046: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 47/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9875\n",
            "Epoch 00047: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "Epoch 48/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 00048: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 49/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9750\n",
            "Epoch 00049: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0724 - accuracy: 0.9750 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 50/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 00050: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 51/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9875\n",
            "Epoch 00051: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0334 - accuracy: 0.9875 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 52/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 00052: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 53/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9875\n",
            "Epoch 00053: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0298 - accuracy: 0.9875 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 54/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 00054: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 55/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 00055: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 56/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9625\n",
            "Epoch 00056: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1423 - accuracy: 0.9625 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 57/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9875\n",
            "Epoch 00057: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0406 - accuracy: 0.9875 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 58/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9875\n",
            "Epoch 00058: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0427 - accuracy: 0.9875 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 59/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9875\n",
            "Epoch 00059: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0280 - accuracy: 0.9875 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 60/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 00060: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 61/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9875\n",
            "Epoch 00061: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 62/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
            "Epoch 00062: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 63/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9750\n",
            "Epoch 00063: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0445 - accuracy: 0.9750 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 64/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9625\n",
            "Epoch 00064: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0751 - accuracy: 0.9625 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 65/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9875\n",
            "Epoch 00065: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0279 - accuracy: 0.9875 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 66/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9875\n",
            "Epoch 00066: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0274 - accuracy: 0.9875 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 67/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 00067: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 68/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9875\n",
            "Epoch 00068: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0261 - accuracy: 0.9875 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 69/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 00069: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 70/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 00070: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 71/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 00071: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 72/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 00072: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 73/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00073: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 74/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9625\n",
            "Epoch 00074: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0748 - accuracy: 0.9625 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 75/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 00075: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 76/80\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9875\n",
            "Epoch 00076: val_accuracy did not improve from 1.00000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0173 - accuracy: 0.9875 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 00076: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "cIhecREwpa3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-2B8tCRppXt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "44310ff3-7ab7-4908-b4a7-091cb0206def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAELCAYAAADTK53JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1xUdf4/8BczMIiKCCQ4SEVSKEqlkml5QdFWzSEofWRhZmua5abV5lcxTcU7rr9qvey6q+nqYrVLXiHvqdkFDUdSidRyMSFHkJsXRC4z5/cHDyYGhrkPc+bwej4ePh7MnNvrnDO8PXw+53zGQxAEAUREJDkyVwcgIiLnYIEnIpIoFngiIoligScikigWeCIiiWKBJyKSKBb4VmTy5MnYuXOnw+d1pdjYWHz33XcOX2+3bt3w66+/AgDmz5+PdevWWTSvtfbs2YNJkybZtCyROZ6uDkCm9e7dW/9zZWUlFAoF5HI5ACA5ORnPPPOMxevauHGjU+aVukWLFjlkPQUFBRg2bBh+/PFHeHrW/eo988wzVp1DImuwwItcdna2/ufY2FgsWbIETz75ZJP5amtr9UWDyNX4eRQHNtG4qZMnT2Lw4MH45z//iQEDBmDOnDm4ceMGpk6div79+6Nv376YOnUqrl27pl9mwoQJSEtLAwDs2LEDL774IlJSUtC3b1/Exsbiq6++smne/Px8jB8/Hr1798Yrr7yC5ORkzJw502huSzJ+9NFHeOGFF9C7d29MmjQJpaWl+um7du3C0KFD0a9fP/z9739v9vicOXMGAwYMgFar1b936NAhxMXFAQDOnj2LcePG4bHHHsPAgQOxaNEiVFdXG11XUlISPvzwQ/3rjRs3YuDAgRg4cCA+//xzg3mPHTuGhIQE9OnTBzExMVizZo1+2ksvvQQA6Nu3L3r37o3s7Gz9sa13+vRpjBkzBtHR0RgzZgxOnz5t8bGx5jiXl5djzpw5GDhwIPr27Ytp06bppx0+fBjx8fHo06cPhg8fjuPHjwNo2hy2Zs0a/XkuKChAt27dkJaWhiFDhmDixIkAgBkzZmDAgAGIjo7G+PHj8fPPP+uXv3v3LlasWIGhQ4ciOjoaL774Iu7evYvXXnsN//73vw32Jy4uDocOHTK6r9Q8Fng3VlxcjBs3buDo0aNYvHgxdDodnnvuORw9ehRHjx6Ft7e3yeaFs2fP4oEHHsCJEycwefJkzJ07F82NXGFq3pkzZ+KRRx7ByZMn8eabb2L37t3NbtOSjBkZGVi+fDkyMzNRU1ODTZs2AQB++eUXJCcnY+XKlfj6669RXl5uULQaevTRR+Hj44MTJ07o30tPT9cXeJlMhjlz5uDEiRP47LPPkJmZiU8++aTZ3PWOHz+OTZs2YdOmTTh48CAyMzMNpvv4+CAlJQWnTp3CP/7xD3z66ac4fPgwACA1NRUAkJWVhezsbIPmN6Cu6E6dOhUTJkzAyZMn8cc//hFTp05FWVmZ2WNj7XGeNWsWKisr8cUXX+C7777DK6+8AqDuPM+ePRuzZs3CqVOnsG3bNnTp0sXscamXlZWFvXv34uOPPwYADB48GAcOHEBmZiZ69Ohh8B9/SkoKfvzxR3z22Wf4/vvv8X//93+QyWRISEjAnj179POdP38eRUVFiImJsTgH1WGBd2MymQwzZsyAQqFAmzZt4O/vjxEjRsDHxwft27fHG2+8gaysrGaXDwkJwfPPPw+5XI5nn30W169fR3FxsVXzXr16FefOndPneOyxxxAbG9vsNi3J+Nxzz+GBBx5AmzZtMHLkSPz0008AgP3792PIkCHo27cvFAoF3nrrLchkzX+ER48ejYyMDADA7du3cfz4cYwePRoAEBUVhV69esHT0xOhoaEYN26cyWNVb9++fXjuuecQERGBtm3b4s033zSY3q9fP3Tr1g0ymQzdu3fH6NGj8f3335tdL1B39X///fcjISEBnp6eUKlU6Nq1K44ePWr22DRm6jgXFRXh+PHjSE5Ohp+fH7y8vPD4448DAD7//HOMGTMGAwYMgEwmQ3BwMMLDwy3KDwDTp09H27Zt0aZNGwDA2LFj0b59eygUCkyfPh3nz5/HrVu3oNPpsH37dsydOxfBwcGQy+Xo06cPFAoFhg0bhsuXL+Py5csAgN27d2PUqFFQKBQW56A6bCRzY/7+/vD29ta/rqysxPLly/H111/jxo0bAICKigpotVp9x2xD99xzj/5nHx8fAMCdO3eMbqu5ecvKyuDn56d/DwCUSiU0Go3R9ViSsVOnTgbbqs9UVFSEzp0766e1bdsWHTt2NLodoO7P+hdeeAHJyck4dOgQevToob8azcvLw4oVK5CTk4PKykpotVr07Nmz2XXVKyoqQlRUlP5146vbM2fOYNWqVfj5559RU1OD6upqjBw50ux669cdEhJi8F5ISAgKCwv1r5s7No2ZOs7Xrl2Dn58f/Pz8miyn0WjsulJueH60Wi0+/PBD7N+/H6Wlpfr/jMvKylBdXY2qqirce++9Tdbh7e2NUaNGYc+ePXjzzTeRkZGB1atX25ypNeMVvBvz8PAweL1p0ybk5eXhv//9L06fPo1t27YBQLPNLo7QqVMn3LhxA5WVlfr3mivu9mYMCgoyaJKprKxEeXl5s/M/+OCDCAkJwfHjx5GRkQGVSqWftnDhQnTt2hUHDhzA6dOn8c4771icoeH+Xb161WD6u+++i2HDhuGrr76CWq3GCy+8oF9v4/NlbN2N16fRaBAcHGw2V2OmjnPnzp1x48YN3Lx5s8lySqUSV65cMbpOHx8fg/N8/fr1JvM03Mf09HR8+eWX2Lx5M9RqNY4cOaLPUH9xkp+fb3Rbzz77LNLT05GZmQkfH58mzVlkGRZ4CamoqIC3tzc6dOiA8vJyrF271unb7NKlC6KiorBmzRpUV1cjOzvboEnBkRlHjBiBY8eO4dSpU6iursbq1auh0+lMLqNSqbBlyxZkZWUZXElXVFSgXbt2aNeuHS5duoRPP/3UogwjR47Ezp078csvv6CysrJJ/oqKCvj5+cHb2xtnz57VNxEBQEBAAGQyWbNFLSYmBpcvX0Z6ejpqa2uxd+9e/PLLLxgyZIhF2RrnaO44BwUFYfDgwUhOTsaNGzdQU1Ojb74ZO3YsduzYgczMTOh0OhQWFuLSpUsAgO7du2Pv3r2oqanBuXPncODAAbMZFAoF/P39UVlZiQ8++EA/TSaTYcyYMVi+fDkKCwuh1WqRnZ2t7+ju3bs3ZDIZVqxYwdtI7cACLyETJ05EVVUV+vfvj3HjxmHQoEEtst1Vq1bhhx9+QL9+/fDRRx/h6aefbra91J6MDz30EObPn4+ZM2di0KBB6NChg0GTgDEqlQpZWVno378/AgIC9O/Pnj0bGRkZ6NOnD95//308/fTTFmWIiYnBxIkTMXHiRDz11FPo37+/wfQFCxZg9erV6N27N9atW4dRo0bpp/n4+OD111/Hiy++iMceeww//PCDwbL+/v5Yv349Nm/ejH79+mHjxo1Yv369QW5LmTvOK1euhKenJ0aNGoUnn3wSW7ZsAQA88sgjWL58OZYtW4bo6Gi89NJL+r8q3n77bVy5cgWPP/441qxZo++wbk5CQgJCQkIwaNAgjB49Gr169TKYPnv2bERERGDs2LF4/PHHsWrVKoP/sOPj43Hx4kXEx8dbvf9Ux4Nf+EGO9vbbb6Nr166YMWOGq6OQG9u1axf+85//WPzXFTXFK3iy29mzZ3HlyhXodDocP34cX375JYYPH+7qWOTGKisr8cknn2DcuHGujuLWeBcN2a24uBjTp09HeXk5OnfujIULF6JHjx6ujkVu6uuvv8b06dPxxBNPGHSMk/XYRENEJFFsoiEikihRNNHodDpUVFTAy8vL7L3CRERURxAE1NTUoF27dkaf6hZFga+oqMDFixddHYOIyC1FRETA19e3yfuiKPBeXl4A6kLaMt5ETk6OwePjYiT2jGLPBzCjI4g9HyD+jGLKV11djYsXL+praGOiKPD1zTIKhcJgbBVr2LpcSxJ7RrHnA5jREcSeDxB/RrHla65pm52sREQSxQJPRCRRLPBERBJltsCnpKQgNjYW3bp1a/ZOF61Wi+TkZAwfPhxPPfWU/qveiIjIdcx2sg4bNgwvv/wyxo8f3+w86enpuHLlCg4ePIjy8nIkJCTgiSeeQGhoqEPDUss4ps7H1n0/obisEvf4++DlUZEYEt30ixlcxd58tizv7G02nt63exCyzhdZPL+zp5vLYwt7Mzp7e433uf719bJKdNpX0ux0U3lb+ndLvnDhwoWmZggJCYGvry+2bNkClUqFwMDAJvN88MEHGDt2LB566CH4+PggPz8fhYWF6NOnj0UhtFotioqKEBQUZNM3sWs0GoNvwtHpBAgCRPXvquYqlJ2VLs9hLt8xdT7Wpp3BrTs1AIA7d2uh/qkQQf4+uL9zB5dnvHi11q58tuyfNcsYO8/mljc2/eeCG7hzt9bi+S2d7oUKs8fQ2jy2nEt7Mzpye5bus7WvG+c1leGBkKbfrmUJc7XT4rFoYmNjsX79ekRERDSZFhcXh6VLl+KRRx4BAGzYsAGFhYWYN2+eRSGrqqr095bacvuRWq1GdHQ0AODUT4VYvOkkdDqLdouIyOX+Mn0QuodZP+6/udopivvg6+Xk5Ni8rFqtBgDsV5dD5gEMfriDo2K1KsfONf0at3pDRHBM7c1ny/LO3qap6dbO7+zpxua3hb0ZW3J79qrPa2obxZpLUJfkOXzbDinwSqUSV69e1V/BN24ysZQjruDTTnyDh+5V4N1XWubbjCzVMKMY1ef7cclBXC+rbDK9k78P3n1lqAuS/U6tVuPHghq78tmyf9YsY+w8m1u+uenWzm/J9CEPdzB7DK3NYwt7Mzpye9bss7Ua5jWVYeATfW1af/0VfHMcUuBHjhyJtLQ0/OEPf0B5eTkOHz6s/5LflqTTCfjfb+UY9th9Lb5tqXh5VCTWpp1BVY1W/563lxwvj4rUv3Z0h2Bjpua3JZ+9y/ftHoQvTxUYLCOXeeBuVS2eeXe30Q44c9tsuHz7tl7wlHugVtt8s6K5+Rvug6ntLfykAL5Glrc2T+Nj1txxa64T0tg2zGU0tk1Lt29qe5bus7Ua5zX3OXBGp6vZTtYlS5Zg3rx5KCoqwoEDB7Bjxw6MHz8eU6ZMwQMPPIDg4GBEREQgOzsby5YtQ1paGl5//XU8+eSTFodwVCfr1eIK7PrqEkY9GYauXTpavR5nsvWvmpZSny8sxA9B/j74paAclXdr0cnfB1Pio/QfuvqOopsVdV+ObKxz6fT5IgT5+yAsxM/o/A2nN2Zqfm9U4Mno7lbna7g9W/bvSuFtDHssFOUV1ai8Wwvftl6o1epwt1pr0TFovM3Gy1fX6ODh4YH2Pl6oqdHVXcX27tLs9ozN33AfrN2etXkab8/Sz0XD17ZkaLxNc58be7fXeJ/rX99p9Lrx9OaOkbnzYu53wxiHdbI6k6M6WY9nF+AvqWr89c9D0LWLbb3SzuIuTTTmTLLiz/dN8/7Q7Pz10y1dfyd/H/xpVKDZjNZuz5blrT0G9mZ09j7Zu35T2zDHURmc/blsyFG/y4447uZqp6SeZL1UcAOechnuDW46bCY5RrGFv8T18zU3v6Ped8XyzsriqmNi7/qtndcZGZz9uXSGlsggrQL/WznClL7w8pTUbonKPf4+Vs3X3PyOet8Vyzsri6uOib3rt3ZeZ2Rw9ufSGVoig9k2+JbgiDZ4pVKJjbtz8GhEJzzes7MTUlrmmDofizadxMe7c3Ao6wr82ikQFuLnsDb45tbfUvn82ilw+nwRtCaeM/D2kmNKfBTCQvyMzi+XecDDA9jyRS4OZV3B1aJb+PuOs/h4dw5kMg/odAIarr5+fd6oMJvRku2ZOmbGlm+4P5YeA1PbtGQb1mYy9bkwt7y1eSw9buZYm8HafbR3e81x1O+yI467udopmQIvb9MRnx/5GSP63Y+H7vV3QkrzzHUQ2vuhsLbD0hn5jHVSmupcsqRjyVRnWMP1WfKLZW9HlrlOWHPH4I4F27RkG9ZksqZj+Y4F58hcHkszWtMJaS6jLZ3n1nwurdlnRxV4Rxz3VtPJWq1QYtm/svD/3hqMiPtcU+Dt7SC0Z/32drY5Ip8l7OmgtKVzyxEdWdZQq9VYt6+kRbdpzT6KvbMfMJ6xpc+jKWI6hq2mk/VSwQ3IZB64X+m6py2d3Wni6g5IR3B0p5mt63HmPrf0NsVwXp2tNeyjM0inwP92A/cF+8LbS+6yDM7uNHF1B6QjOLrTzNb1OHOfW3qbYjivztYa9tEZRDUWjT0uFZSjd7cgl2Yw/ZRkEQD7hmG15ClMa/M1fHqw8ROY9Zz9JGpj1uyTOfYeM3fYpiv2saWHvXXFU6CO4OqhtyXRyXrxfwU4cqYcw/veh273Wz8im6OY6jTRaDS4qNFa9RSoNeu3JZ8lHZCOfBLVls6whmzp3HJER5Y1NBqN2adtHc2afXREB6G9nf3mGMvojKdAHZnPGGcfJ8B87ZTEFbymtO4Ahoe6fniCIdH3NvuLvHXfTyavXAGgqkaLrft+anYdptZvbb5JSw7qx6ZubvvGMpvKaMn8xvbhDZv3yDx7j5k7bLMlt2ftZ8JRrP3supqrjlNDkmiD15TWnegHQlw/nK0pLd3BaOt2LHlq01lPXZL4ieEciyGDOWLIKI0CX1aNLp3aoW0bL1dHMamlOxht3Y4lT20666lLEj8xnGMxZDBHDBmlUeBLaxAustEjjXl5VKTZu3yc3TlmLo+xIU7NzWPtOsm9ieEciyGDOWLI6PZt8DcrqnHjjhbhoeIaPdKYhu3azvpyZWu+CLhhnutllehkZP3GMpu6E8Da+R3B1XcqtDauOMdizGCOGDK6fYH/32/lAOAWV/CAdR2M9b3w9R0118sqsTbtjH49xqbvzfxVv7yx1w2Xb5jH1NN51nbgtWSHn7ljRM7hio5rMWYwx9UZ3b6JpvTmXchlQFc3uIK3lqle+Oamm9NweSkwd4yIWjO3v4If+GgX1N66Ct+2CldHcThbx/G2db3uSAx3KhCJldtfwSu85AjsIO67Z2xl6zjetq7XHYnhTgUisXL7K3hns7aT05GdKOYeQbfksf/GxHangSXqj7GlX2ht7T6yk5akigXeBFs6OR3ZwWeuF96Su3KsGetGjMwdY3vvVGAnLUkZC7wJ5h41bolHkc31wltyV44zhwFwNluHPnDk+oncldu3wTtTS3xZMZkm9jH2icSMBd6ElviyYjJN7GPsE4kZC7wJxh41bjgG9d2qWnjKPQym29uJeUydj0lLDuKZd3dj0pKDOKbOt3ldUuDsx73F8Dg5kbOwDd6Exh147dt6obKqVj9M6a07NZDLPODb1gu379TY3YnJDr+mLBlOwVHrd9eOaKLmsMCbYW4Maq1OQBtvT3yy+Gm7t8UOP+MsGU7BEesnkho20ViBHX5E5E5Y4K3ADj8icics8FZghx8RuRO2wVvB2R1y7PAjIkdigbeSszvk2OFHRI7CJhoiIoligScikiiLmmjy8vKQlJSE8vJydOzYESkpKQgLCzOY5/r165g/fz4KCgpQW1uL119/HfHx8c7I7FQcOpaIpMKiK/gFCxYgMTERBw4cQGJiIubPn99knhUrViAqKgrp6enYtm0bPvzwQ2g0GocHdqb6J0mvl1VCwO9Pkrb24QKIyD2ZLfAlJSXIzc2FSqUCAKhUKuTm5qK0tNRgvvPnz2PQoEEAgICAAHTv3h379u1zQmTn4fd7EpGUmG2i0Wg0CA4Ohlxed3+2XC5HUFAQNBoNAgIC9PP17NkTe/fuxcMPP4yCggJkZ2cjNDTUqjA5OTlWxv+dWq22edl615t5YvR6WaVD1u+IdTiT2PMBzOgIYs8HiD+j2PPVc9htkklJSVi2bBni4+MREhKCJ554Qv+fgqWioqLg7e1t9bYdNUZJp30lRot8J38fu9fvrHFUHEXs+QBmdASx5wPEn1FM+aqqqkxeGJst8EqlEoWFhdBqtZDL5dBqtSgqKoJSqTSYLyAgAKtWrdK/njJlCh588EE7orc8R3y/JxGRWJgt8IGBgYiMjERGRgbi4+ORkZGByMhIg+YZACgrK4Ovry88PT2RmZmJixcvYvXq1U4LbonGd8SY+35SZzxJauoLo4mInMmiJpqFCxciKSkJf/vb39ChQwekpKQAqLtKnzFjBh5++GGcPXsWS5cuhUwmg7+/P9avXw8fH9cNkmVsbPW9mb/qpzc31rojnyTl+O5E5EoWFfjw8HCkpaU1eX/Dhg36n2NiYhATE+O4ZHYydkdMY84ea53juxORK0n2SVZLx1B35ljrHN+diFxJsgXe0jHUnTnWOsd3JyJXkmyBNza2emPOvkOG47sTkStJdrhgY3fEmLuLxpkZnPGF0UREpki2wAPG74h5w0UZxPRwBBG1DpJtoiEiau1Y4ImIJEpSTTQcy52I6HeSKfB8apSIyJBkmmg4ljsRkSHJFHg+NUpEZEgyBZ5PjRIRGZJMgedTo0REhiTTyeqMsdyJiNyZZAo84Nix3ImI3J1kmmiIiMgQCzwRkUSxwBMRSRQLPBGRRLHAExFJFAs8EZFEscATEUkUCzwRkURJ6kEnW3AMeSKSqlZd4DmGPBFJWatuouEY8kQkZa26wHMMeSKSslZd4DmGPBFJWasu8BxDnoikrFV3snIMeSKSslZd4AGOIU9E0tWqm2iIiKTMoiv4vLw8JCUloby8HB07dkRKSgrCwsIM5ikpKcGcOXOg0WhQW1uLfv36Yd68efD0bPV/JBARuYRFV/ALFixAYmIiDhw4gMTERMyfP7/JPOvXr0d4eDjS09OxZ88e/Pjjjzh48KDDAxMRkWXMFviSkhLk5uZCpVIBAFQqFXJzc1FaWmown4eHByoqKqDT6VBdXY2amhoEBwc7JzUREZlltsBrNBoEBwdDLq+7nVAulyMoKAgajcZgvmnTpiEvLw8DBw7U/4uOjnZOaiIiMsthDeT79+9Ht27dsGXLFlRUVGDKlCnYv38/Ro4cafE6cnJybN6+Wq22edmWIvaMYs8HMKMjiD0fIP6MYs9Xz2yBVyqVKCwshFarhVwuh1arRVFREZRKpcF8qampWLZsGWQyGXx9fREbG4uTJ09aVeCjoqLg7e1t9U6o1WrR/7Ug9oxizwcwoyOIPR8g/oxiyldVVWXywthsE01gYCAiIyORkZEBAMjIyEBkZCQCAgIM5gsNDcXx48cBANXV1cjMzMRDDz1kT3YiIrKDRXfRLFy4EKmpqRgxYgRSU1ORnJwMAJgyZQrOnTsHAHjvvfegVqsRFxeHhIQEhIWF4fnnn3deciIiMsmiNvjw8HCkpaU1eX/Dhg36n++77z5s3rzZccmIiMgufJKViEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKI8LZkpLy8PSUlJKC8vR8eOHZGSkoKwsDCDeWbNmoULFy7oX1+4cAHr1q3DsGHDHBqYiIgsY1GBX7BgARITExEfH4/du3dj/vz52Lp1q8E8K1eu1P98/vx5TJw4EYMGDXJsWiIispjZJpqSkhLk5uZCpVIBAFQqFXJzc1FaWtrsMp9//jni4uKgUCgcl5SIiKxitsBrNBoEBwdDLpcDAORyOYKCgqDRaIzOX11djfT0dIwZM8axSYmIyCoWNdFY4/DhwwgJCUFkZKTVy+bk5Ni8XbVabfOyLUXsGcWeD2BGRxB7PkD8GcWer57ZAq9UKlFYWAitVgu5XA6tVouioiIolUqj82/fvt3mq/eoqCh4e3tbvZxarUZ0dLRN22wpYs8o9nwAMzqC2PMB4s8opnxVVVUmL4zNNtEEBgYiMjISGRkZAICMjAxERkYiICCgybzXrl2DWq1GXFycHZGJiMgRLLoPfuHChUhNTcWIESOQmpqK5ORkAMCUKVNw7tw5/Xw7d+7E0KFD4efn55y0RERkMYva4MPDw5GWltbk/Q0bNhi8fuONNxyTioiI7MYnWYmIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkyqICn5eXh3HjxmHEiBEYN24cLl++bHS+vXv3Ii4uDiqVCnFxcSguLnZkViIisoKnJTMtWLAAiYmJiI+Px+7duzF//nxs3brVYJ5z585h7dq12LJlCzp16oRbt25BoVA4JTQREZln9gq+pKQEubm5UKlUAACVSoXc3FyUlpYazPevf/0LkyZNQqdOnQAAvr6+8Pb2dkJkIiKyhNkreI1Gg+DgYMjlcgCAXC5HUFAQNBoNAgIC9PNdunQJoaGhGD9+PO7cuYOnnnoKb7zxBjw8PCwOk5OTY8Mu1FGr1TYv21LEnlHs+QBmdASx5wPEn1Hs+epZ1ERjCa1WiwsXLmDz5s2orq7G5MmTERISgoSEBIvXERUVZdNVv1qtRnR0tNXLtSSxZxR7PoAZHUHs+QDxZxRTvqqqKpMXxmabaJRKJQoLC6HVagHUFfKioiIolUqD+UJCQjBy5EgoFAq0b98ew4YNw9mzZ+2MT0REtjJb4AMDAxEZGYmMjAwAQEZGBiIjIw2aZ4C6tvlvvvkGgiCgpqYGJ06cQPfu3Z2TmoiIzLLoNsmFCxciNTUVI0aMQGpqKpKTkwEAU6ZMwblz5wAAo0ePRmBgIJ5++mkkJCTgwR3CHWcAAAdhSURBVAcfxNixY52XnIiITLKoDT48PBxpaWlN3t+wYYP+Z5lMhjlz5mDOnDmOS0dERDbjk6xERBLFAk9EJFEs8EREEsUCT0QkUSzwREQSxQJPRCRRLPBERBLFAk9EJFEs8EREEsUCT0QkUSzwREQSxQJPRCRRLPBERBLFAk9EJFEs8EREEsUCT0QkUSzwREQSxQJPRCRRLPBERBLFAk9EJFEs8EREEsUCT0QkUZ6uDmCPY+p8bN33E66XVaLTvhK8PCoSQ6LvdXUsIiJRcNsCf0ydj7VpZ1BVowUAXC+rxNq0MwDAIk9EBDduotm67yd9ca9XVaPF1n0/uSgREZG4uG2BLy6rtOp9IqLWxm0L/D3+Pla9T0TU2rhtgX95VCS8veQG73l7yfHyqEgXJSIiEhe37WSt70jV30Xj78O7aIiIGnDbAg/UFfkh0fdCrVYjOjra1XGIiETFbZtoiIjINBZ4IiKJYoEnIpIoFngiIokSRSerIAgAgOrqapvXUVVV5ag4TiP2jGLPBzCjI4g9HyD+jGLJV18z62toYx5Cc1Na0K1bt3Dx4kVXxyAicksRERHw9fVt8r4oCrxOp0NFRQW8vLzg4eHh6jhERG5BEATU1NSgXbt2kMmatriLosATEZHjsZOViEiiWOCJiCSKBZ6ISKJY4ImIJIoFnohIoljgiYgkigWeiEiiRDFUgT3y8vKQlJSE8vJydOzYESkpKQgLC3NZnpSUFBw4cAC//fYb0tPTERERIaqcZWVlmDVrFq5cuQKFQoH7778fixYtQkBAAH744QfMnz8fVVVV6NKlC/7yl78gMDCwxTMCwLRp01BQUACZTIa2bdvi/fffR2RkpGiOY721a9dizZo1+nMtpmMYGxsLhUIBb29vAMDMmTMxaNAgUWWsqqrCsmXLkJmZCW9vb/Tq1QuLFy8WxXkuKCjAn/70J/3rW7du4fbt2/j+++9Fkc8igpubMGGCsGvXLkEQBGHXrl3ChAkTXJonKytLuHr1qjB06FDhwoUL+vfFkrOsrEw4ceKE/vWKFSuEOXPmCFqtVhg+fLiQlZUlCIIgrFu3TkhKSnJJRkEQhJs3b+p/PnTokJCQkCAIgniOoyAIQk5OjvDqq6/qz7XYjmHjz6AgCKLLuHjxYmHp0qWCTqcTBEEQrl+/LgiCuM5zvSVLlgjJycmCIIgznzFuXeCLi4uF6Ohooba2VhAEQaitrRWio6OFkpISFycz/OUSc879+/cLEydOFM6cOSOMHj1a/35JSYnQq1cvFyb73c6dO4Vnn31WVMexqqpKeP7554X8/Hz9uRbbMTRW4MWU8fbt20J0dLRw+/Ztg/fFdJ7rVVVVCf369RNycnJEma85bt1Eo9FoEBwcDLm87su35XI5goKCoNFoEBAQ4OJ0vxNrTp1Oh08//RSxsbHQaDQICQnRTwsICIBOp9P/CeoKc+fOxbfffgtBELBx40ZRHce//vWveOaZZxAaGqp/T4zHcObMmRAEAdHR0fjzn/8sqoz5+fno2LEj1q5di5MnT6Jdu3Z466230KZNG9Gc53pHjhxBcHAwevbsiZycHNHlaw47WVuxxYsXo23btnjppZdcHcWopUuX4tixY3jnnXewcuVKV8fRy87ORk5ODhITE10dxaRt27Zhz5492L59OwRBwKJFi1wdyYBWq0V+fj569OiBHTt2YObMmZg+fTru3Lnj6mhNbN++HWPGjHF1DKu5dYFXKpUoLCyEVqsFUPeBKSoqglKpdHEyQ2LMmZKSgl9//RUfffQRZDIZlEolrl69qp9eWloKmUzmsivPhhISEnDy5El07txZFMcxKysLly5dwrBhwxAbG4tr167h1Vdfxa+//iqqY1h/XBQKBRITE3H69GlRnWelUglPT0+oVCoAwKOPPgp/f3+0adNGFOe5XmFhIbKyshAXF6fPLaZ8prh1gQ8MDERkZCQyMjIAABkZGYiMjBTdn0liy/nBBx8gJycH69atg0KhAABERUXh7t27OHXqFADgs88+w8iRI12Sr6KiAhqNRv/6yJEj8PPzE81xfO211/DNN9/gyJEjOHLkCDp37oyPP/4YkydPFs0xvHPnDm7dugWgbkjZvXv3IjIyUlTnOSAgAP369cO3334LoO5Os5KSEoSFhYniPNfbuXMnYmJi4O/vD0B8v8+muP1wwZcuXUJSUhJu3ryJDh06ICUlBV27dnVZniVLluDgwYMoLi6Gv78/OnbsiC+++EI0OX/++WeoVCqEhYWhTZs2AIDQ0FCsW7cOp0+fxoIFCwxun7vnnntaPGNxcTGmTZuGyspKyGQy+Pn5Yfbs2ejZs6dojmNDsbGxWL9+PSIiIkRzDPPz8zF9+nRotVrodDqEh4dj3rx5CAoKEk3G+pzvvfceysvL4enpibfffhsxMTGiOs8jRozA3LlzMXjwYP17YspnitsXeCIiMs6tm2iIiKh5LPBERBLFAk9EJFEs8EREEsUCT0QkUSzwREQSxQJPRCRRLPBERBL1/wF3L0Xpm6qYVwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import some of library that we need to look the confusion matrix, recall, f1_score, and accuracy score to look how much your model is well\n",
        "import numpy as np \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')"
      ],
      "metadata": {
        "id": "X5uCatTrpeBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model to evaluate more depth\n",
        "from keras.models import load_model\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/Kecerdasan Buatan Pak Galih /7. Transfer Learning /model_drop_batch_weight_from_callback_2.h5'\n",
        "model = load_model(model_path)"
      ],
      "metadata": {
        "id": "jdre7VdppnC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_metrics(y_true, y_pred):\n",
        "    accuracy=accuracy_score(y_true, y_pred)\n",
        "    precision=precision_score(y_true, y_pred,average='weighted')\n",
        "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"f1Score : {}\".format(f1Score))\n",
        "    cm=confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    return accuracy, precision, f1Score\n",
        "\n",
        "height=224; width=224\n",
        "batch_size=20\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "TESTING_DIR = '/tmp/Dataset/Validation'\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TESTING_DIR,\n",
        "                                                  batch_size=batch_size,                                                             \n",
        "                                                  target_size=(height, width),\n",
        "                                                  class_mode= None,\n",
        "                                                  shuffle=False\n",
        "                                                  )\n",
        "\n",
        "predictions = model.predict_generator(generator=test_generator)\n",
        "yPredictions = predictions > 0.5\n",
        "true_classes = test_generator.classes\n",
        "class_names = test_generator.class_indices\n",
        "Cmatrix_test = confusion_matrix(true_classes, yPredictions)\n",
        "\n",
        "testAcc,testPrec, testFScore = my_metrics(true_classes, yPredictions)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "ax= plt.subplot()\n",
        "data = np.asarray(Cmatrix_test).reshape(2,2)\n",
        "sns.heatmap(data,annot=True, fmt='',ax=ax, cmap=plt.cm.Reds)\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels') \n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(class_names)   \n",
        "ax.yaxis.set_ticklabels(class_names)\n",
        "plt.title('Confusion Matrix Test',fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "asQ5rNVIpqPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_classes, yPredictions, target_names=class_names))"
      ],
      "metadata": {
        "id": "Fo0mbZPMpsDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}